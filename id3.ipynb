{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ID3: Iterative Dichotomiser 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ***Author: Tungon Dugi(PhD NIT AR)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Experiment no. ?:**\n",
    " - **Write a program to demonstrate the working of the decision tree based ID3 algorithm. Use an appropriate data set for building the decision tree and apply this knowledge to classify a new sample.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Dataset name: Play Tennis***\n",
    " - ***available on Kaggle:ðŸ‘‰https://github.com/repleeka/temporary/blob/main/play_tennis.csv***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import pandas as pd # for manupulating csv file\n",
    "import numpy as np # for mathematical calculations\n",
    "# from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. Reading and loading the dataset**\n",
    " - **We are going to read the dataset (csv file) and load it into pandas dataframe.**\n",
    " - **You can see below, ***df*** is our dataframe.**\n",
    " - **With the ***head()*** method of the dataframe we can view the first 5 rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('play_tennis.csv')\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_counts = df['Play Tennis'].value_counts()\n",
    "play_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2. Calculating the entropy of the whole dataset**\n",
    "\n",
    "***Total row = 12***\n",
    " - ***Row with \"Yes\" = 6***\n",
    " - ***Row with \"No\" = 6***\n",
    " - ***Complete entropy of the dataset is:***\n",
    "    - ***H(s) = -p(Yes) * log(p(Yes)) - p(No) * log(p(No))***\n",
    "         - ***= -(6/12) * log(6/12) - (6/12) * log(6/12)***\n",
    "         - ***= 1***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **3. Calculating entropy of the feature Outlook**:\n",
    " - ***Categorical values of Outlook - Sunny, Overcast and Rain***\n",
    "   - ***Total count of row containing:***\n",
    "     - ***Sunny = 2***\n",
    "     - ***Sunny & Yes = 1***\n",
    "     - ***Sunny & No = 1***\n",
    "   - ***H(Outlook=Sunny) = -(1/2)xlog(1/2)-(1/2)xlog(1/2) = 1***\n",
    "   - ***Total count of row containing:***  \n",
    "     - ***Rain = 8***\n",
    "     - ***Rain & Yes = 5***\n",
    "     - ***Rain & No = 3***\n",
    "   -  ***H(Outlook=Rain) = -(5/8)xlog(5/8)-(3/8)xlog(3/8) = 0.6858***\n",
    "   - ***Total count of row containing:***  \n",
    "     - ***Overcast = 2***\n",
    "     - ***Overcast & Yes = 0***\n",
    "     - ***Overcast & No = 2***\n",
    "   - ***H(Outlook=Overcast) = -(0/2)xlog(0/2)-(2/2)xlog(2/2) = 0***\n",
    "\n",
    "**Note**: ***We have to do the same for all features like Wind, Humidity etc.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **4. Calculating information gain for a feature**\n",
    " - After calculating entropy, we have to calculate the information gain of that feature. \n",
    "   - In math, first, we have to calculate the information of that feature like this: (for the feature Outlook)\n",
    "\n",
    "\n",
    "   - ***I(Outlook) = p(Sunny) * H(Outlook=Sunny) + p(Rain) * H(Outlook=Rain) + p(Overcast) * H(Outlook=Overcast)***\n",
    "      - ***= (2/12) x 1 + (8/12) x 0.6858 + (2/12)***\n",
    "      - ***= 0.6239***\n",
    " - Then, we have to subtract this from the total entropy of the dataset which is the information gain of the feature.\n",
    "      - ***Information Gain = H(S) - I(Outlook)***\n",
    "        - = ***1 - 0.6239***\n",
    "        - = ***0.3761***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **5. Finding the most informative feature (feature with highest information gain)**\n",
    "\n",
    " - ***Like Outlook feature, We have to calculate information gain for every feature in the dataset.***\n",
    " - ***Then we have to select the feature with the highest information gain.*** \n",
    " - ***After calculating mathematically we will find the values like below***:\n",
    "\n",
    " - ***Information gain***:\n",
    "   - ***Outlook = 0.2467 (Highest value)***\n",
    "   - ***Temperature = 0.0292***\n",
    "   - ***Humidity = 0.1518***\n",
    "   - ***Wind = 0.0481***\n",
    "\n",
    "**As the feature Outlook has the highest value, so it will be selected for our tree node.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **6. Python implementation for above calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_entropy(data, label):\n",
    "    class_counts = data[label].value_counts()\n",
    "    total_instances = len(data)\n",
    "    entropy = 0\n",
    "\n",
    "    for count in class_counts:\n",
    "        probability = count / total_instances\n",
    "        entropy -= probability * np.log2(probability)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def calc_info_gain(data, feature, label):\n",
    "    total_entropy = calc_entropy(data, label)\n",
    "    feature_values = data[feature].unique()\n",
    "    weighted_entropy = 0\n",
    "\n",
    "    for value in feature_values:\n",
    "        subset = data[data[feature] == value]\n",
    "        subset_entropy = calc_entropy(subset, label)\n",
    "        probability = len(subset) / len(data)\n",
    "        weighted_entropy += probability * subset_entropy\n",
    "\n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "\n",
    "def id3(data, features, label):\n",
    "    # Base case: If all instances have the same class, return a leaf node with that class\n",
    "    if len(data[label].unique()) == 1:\n",
    "        return data[label].iloc[0]\n",
    "\n",
    "    # Base case: If there are no features left to split on, return the majority class\n",
    "    if len(features) == 0:\n",
    "        return data[label].mode().iloc[0]\n",
    "\n",
    "    # Choose the best feature to split on based on information gain\n",
    "    info_gains = [calc_info_gain(data, feature, label) for feature in features]\n",
    "    best_feature_index = np.argmax(info_gains)\n",
    "    best_feature = features[best_feature_index]\n",
    "\n",
    "    # Create a tree node with the best feature\n",
    "    tree = {best_feature: {}}\n",
    "\n",
    "    # Remove the chosen feature from the list of available features\n",
    "    remaining_features = [\n",
    "        feature for feature in features if feature != best_feature]\n",
    "\n",
    "    # Recursively build the tree for each value of the chosen feature\n",
    "    for value in data[best_feature].unique():\n",
    "        subset = data[data[best_feature] == value]\n",
    "        tree[best_feature][value] = id3(subset, remaining_features, label)\n",
    "\n",
    "    return tree\n",
    "\n",
    "# List of features (excluding the target variable 'Play Tennis')\n",
    "features = ['Outlook', 'Temperature', 'Humidity', 'Windy']\n",
    "\n",
    "# Build the decision tree using ID3 algorithm\n",
    "decision_tree = id3(df, features, 'Play Tennis')\n",
    "\n",
    "# Print the resulting decision tree\n",
    "print(decision_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **6. Implementing using ID3 algorithm in SkLearn**\n",
    "\n",
    " - the ID3 (Iterative Dichotomiser 3) algorithm, can be considered as feature selection algorithms. \n",
    " - Feature selection refers to the process of selecting a subset of relevant features from the original set of features in a dataset\n",
    " - The goal is to choose the most informative features that contribute the most to the prediction or classification task.\n",
    "\n",
    " - In the context of decision tree algorithms like ID3, feature selection is an integral part of the tree-building process.\n",
    " - The algorithm selects features for splitting nodes based on criteria such as information gain (for classification problems) or reduction in mean squared error (for regression problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **a. Label Encoding for categorical features**\n",
    " - ***Label encoding is converting categories (like Rainy or Overcast, Cool or Hot, etc.) into numbers, making it easier for machine learning algorithms to understand and work with them.***\n",
    " - ***Categorical features are variables that represent different groups or types of data.***\n",
    "\n",
    " 1. **Outlook**: Categorical feature representing different weather conditions (Rainy, Overcast, Sunny).\n",
    " 2. **Temperature**: Categorical feature representing different temperature levels (Cool, Hot, Mild).\n",
    " 3. **Humidity**: Categorical feature representing humidity levels (High, Normal).\n",
    " 4. **Windy**: Categorical feature indicating whether it is windy or not (False, True).\n",
    " 5. **Play Tennis**: Categorical target variable indicating whether tennis is played or not (No, Yes).\n",
    "\n",
    "These features have discrete and distinct categories, and they are not numeric in nature. They need to be encoded into a numerical format for machine learning algorithms to process them effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "df_encoded = df.apply(label_encoder.fit_transform)\n",
    "df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **b. Separate features and target variable**\n",
    " - In the given dataset, the features are the columns ***'Outlook'***, ***'Temperature'***, ***'Humidity'***, and ***'Windy'***, while the target variable is the column ***'Play Tennis'***. \n",
    "  - The goal is to separate these features and the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = df_encoded.drop('Play Tennis', axis=1)\n",
    "y = df_encoded['Play Tennis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **c. Splitting the Dataset into Training and Testing dataset**\n",
    " - ***test_size*** is a parameter that defines the proportion of the dataset to include in the testing split. \n",
    " - ***random_state*** is an optional parameter that controls the randomness of the data splitting. When you set random_state to a specific value (e.g., an integer), it ensures reproducibility. The same random split will be obtained if you use the same random seed.\n",
    "  - If you omit ***random_state*** or set it to `None`, the split will be different every time you run the code.\n",
    "  - Also ***random_state*** value greatly affects the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **d. Model creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree classifier with the \"entropy\" criterion\n",
    "dt_classifier = DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **e. Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier on the training data\n",
    "dt_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **f. Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target variable on the test set\n",
    "y_pred = dt_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **g. Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **h.Display the Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the decision tree\n",
    "from sklearn.tree import export_text\n",
    "text_representation = export_text(dt_classifier, feature_names=list(X.columns))\n",
    "print(\"Decision Tree Rules:\")\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "fig = plt.figure(figsize=(15, 20))\n",
    "tree.plot_tree(dt_classifier, feature_names=list(X.columns), class_names=label_encoder.classes_,filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The structure of the decision tree is influenced not only by the information gain but also by the order of features considered during the tree-building process. The ID3 algorithm, and its variations like C4.5 (used by scikit-learn), typically use a heuristic to determine the order of feature selection.\n",
    "\n",
    " - The decision tree-growing algorithm, in scikit-learn's implementation, selects the feature with the highest information gain or Gini impurity at each step. However, if two or more features have the same information gain, the algorithm might choose the feature that appears earlier in the dataset.\n",
    "\n",
    " - In your provided JSON representation of the decision tree, it seems that 'Temperature' is chosen before 'Outlook' even though 'Outlook' has higher information gain. This ordering might be due to the specific heuristic or tie-breaking rule used in the algorithm.\n",
    "\n",
    " - In practice, the decision tree's structure might vary slightly depending on the exact implementation details and the specific version of the machine learning library used. The key point is that the algorithm selects features at each step based on their information gain or Gini impurity, but when multiple features have the same score, other factors, such as the order they appear in the dataset, can influence the decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The structure of a decision tree is indeed influenced by both the information gain and the order of features considered during the tree-building process.**\n",
    "\n",
    " - ***Information Gain (IG)***:\n",
    "     - Information gain is a crucial criterion for selecting the best feature to split the data at each node of the decision tree.\n",
    "     - It measures how much uncertainty or impurity in the target variable (e.g., classification labels) is reduced after splitting the data based on a particular feature.\n",
    "     - Features with higher information gain are preferred because they lead to more effective splits, resulting in better separation of classes.\n",
    "\n",
    " - ***Order of Feature Consideration***:\n",
    "     - The order in which features are considered during tree-building impacts the treeâ€™s structure.\n",
    "     - Decision trees typically use a greedy approach, meaning they select the best feature at each node based on information gain without considering future nodes.\n",
    "     - The order of feature consideration affects the treeâ€™s depth, shape, and interpretability.\n",
    "     - Features considered early in the process may have a greater impact on the overall tree structure.\n",
    "\n",
    " - In summary, while information gain guides feature selection, the order in which features are evaluated plays a role in shaping the decision tree. Itâ€™s essential to strike a balance between maximizing information gain and maintaining a manageable tree size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
